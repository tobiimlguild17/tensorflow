{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "\n",
    "\n",
    "class SVHN:\n",
    "\n",
    "    def __init__(self, file_path, n_classes, use_extra=False, gray=False):\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # # Load Train Set\n",
    "        train = sio.loadmat(file_path + \"/train_32x32.mat\")\n",
    "        self.train_labels = self.__one_hot_encode(train['y'])\n",
    "        self.train_examples = train['X'].shape[3]\n",
    "        self.train_data = self.__store_data(train['X'].astype(\"float32\"), self.train_examples, gray)\n",
    "\n",
    "        # Load Test Set\n",
    "        test = sio.loadmat(file_path + \"/test_32x32.mat\")\n",
    "        self.test_labels = self.__one_hot_encode(test['y'])\n",
    "        self.test_examples = test['X'].shape[3]\n",
    "        self.test_data = self.__store_data(test['X'].astype(\"float32\"), self.test_examples, gray)\n",
    "\n",
    "        # Load Extra dataset as additional training data if necessary\n",
    "        if use_extra:\n",
    "            extra = sio.loadmat(file_path + \"/extra_32x32.mat\")\n",
    "            self.train_labels = np.append(self.train_labels, self.__one_hot_encode(extra['y']), axis=0)\n",
    "            extra_examples = extra['X'].shape[3]\n",
    "            self.train_examples += extra_examples\n",
    "            self.train_data = np.append(self.train_data, self.__store_data(extra['X'].astype(\"float32\"),\n",
    "                                                                           extra_examples, gray), axis=0)\n",
    "            # shuffle values\n",
    "            idx = np.arange(self.train_data.shape[0])\n",
    "            self.train_data = self.train_data[idx]\n",
    "            self.train_labels = self.train_labels[idx]\n",
    "\n",
    "    def __one_hot_encode(self, data):\n",
    "        \"\"\"Creates a one-hot encoding vector\n",
    "            Args:\n",
    "                data: The data to be converted\n",
    "            Returns:\n",
    "                An array of one-hot encoded items\n",
    "        \"\"\"\n",
    "        n = data.shape[0]\n",
    "        one_hot = np.zeros(shape=(data.shape[0], self.n_classes))\n",
    "        for s in range(n):\n",
    "            temp = np.zeros(self.n_classes)\n",
    "\n",
    "            num = data[s][0]\n",
    "            if num == 10:\n",
    "                temp[0] = 1\n",
    "            else:\n",
    "                temp[num] = 1\n",
    "\n",
    "            one_hot[s] = temp\n",
    "\n",
    "        return one_hot\n",
    "\n",
    "    def __store_data(self, data, num_of_examples, gray):\n",
    "        d = []\n",
    "\n",
    "        for i in range(num_of_examples):\n",
    "            if gray:\n",
    "                d.append(self.__rgb2gray(data[:, :, :, i]))\n",
    "            else:\n",
    "                d.append(data[:, :, :, i])\n",
    "\n",
    "        return np.asarray(d)\n",
    "\n",
    "    def __rgb2gray(self, rgb):\n",
    "        return np.dot(rgb[..., :3], [0.299, 0.587, 0.114]).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73257, 32, 32)\n",
      "(73257, 32, 32, 1)\n",
      "(26032, 32, 32, 1)\n",
      "(26032, 10)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f99a6241ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "svhn = SVHN('svhn', 10, use_extra=False, gray=True)\n",
    "print(svhn.train_data.shape)\n",
    "image_size = 32\n",
    "num_channels = 1\n",
    "num_labels = 10\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  #labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset\n",
    "\n",
    "svhn.train_data = reformat(svhn.train_data, svhn.train_labels)\n",
    "svhn.test_data = reformat(svhn.test_data, svhn.test_labels)\n",
    "print(svhn.train_data.shape)\n",
    "print(svhn.test_data.shape)\n",
    "print(svhn.test_labels.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(svhn.test_data[0,:,:,0])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 32\n",
    "num_channels = 1\n",
    "num_labels = 10\n",
    "num_test_samples = 10000\n",
    "\n",
    "batch_size = 64\n",
    "patch_size = 5\n",
    "depth = 32\n",
    "depth2 = 64\n",
    "num_hidden = 128\n",
    "num_hidden2 = 64\n",
    "pool_size = 3\n",
    "pool_stride = 2\n",
    "padding = 'SAME'\n",
    "\n",
    "image_size_after = image_size // 4\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(svhn.test_data[:num_test_samples])\n",
    "  tf_test_dataset = tf.constant(svhn.test_data[:num_test_samples])\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.01))\n",
    "  layer1_biases = tf.Variable(tf.constant(0.1, shape=[depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth2], stddev=0.01))\n",
    "  layer2_biases = tf.Variable(tf.constant(0.1, shape=[depth2]))\n",
    "\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size_after * image_size_after * depth2, num_hidden], stddev=0.01))\n",
    "  layer3_biases = tf.Variable(tf.constant(0.5, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_hidden2], stddev=0.01))\n",
    "  layer4_biases = tf.Variable(tf.constant(0.5, shape=[num_hidden2]))\n",
    "  layer5_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden2, num_labels], stddev=0.01))\n",
    "  layer5_biases = tf.Variable(tf.constant(0.0, shape=[num_labels]))\n",
    " \n",
    "  # Model.\n",
    "  def model(data, keep_prob=1):\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding=padding)\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    pool = tf.nn.max_pool(hidden, [1, pool_size, pool_size, 1],[1, pool_stride, pool_stride, 1], padding=padding)\n",
    "    \n",
    "    conv = tf.nn.conv2d(pool, layer2_weights, [1, 1, 1, 1], padding=padding)\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    pool = tf.nn.max_pool(hidden,[1, pool_size, pool_size, 1],[1, pool_stride, pool_stride, 1], padding=padding)\n",
    "\n",
    "    shape = pool.get_shape().as_list()\n",
    "    reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    \n",
    "    hidden = tf.nn.dropout(tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases), keep_prob)\n",
    "\n",
    "    hidden = tf.nn.dropout(tf.nn.relu(tf.matmul(hidden, layer4_weights) + layer4_biases), keep_prob)\n",
    "    \n",
    "    return tf.matmul(hidden, layer5_weights) + layer5_biases\n",
    "\n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset, keep_prob=0.8)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.304611\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 15.9%\n",
      "Minibatch loss at step 50: 2.353894\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 19.7%\n",
      "Minibatch loss at step 100: 2.088325\n",
      "Minibatch accuracy: 21.9%\n",
      "Validation accuracy: 21.1%\n",
      "Minibatch loss at step 150: 1.976367\n",
      "Minibatch accuracy: 31.2%\n",
      "Validation accuracy: 32.2%\n",
      "Minibatch loss at step 200: 1.911067\n",
      "Minibatch accuracy: 40.6%\n",
      "Validation accuracy: 40.2%\n",
      "Minibatch loss at step 250: 1.696423\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy: 51.4%\n",
      "Minibatch loss at step 300: 1.263157\n",
      "Minibatch accuracy: 57.8%\n",
      "Validation accuracy: 56.7%\n",
      "Minibatch loss at step 350: 1.060448\n",
      "Minibatch accuracy: 64.1%\n",
      "Validation accuracy: 62.7%\n",
      "Minibatch loss at step 400: 1.236989\n",
      "Minibatch accuracy: 51.6%\n",
      "Validation accuracy: 65.5%\n",
      "Minibatch loss at step 450: 1.103319\n",
      "Minibatch accuracy: 57.8%\n",
      "Validation accuracy: 68.3%\n",
      "Minibatch loss at step 500: 1.040842\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy: 70.5%\n",
      "Minibatch loss at step 550: 1.072103\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy: 68.8%\n",
      "Minibatch loss at step 600: 0.864113\n",
      "Minibatch accuracy: 67.2%\n",
      "Validation accuracy: 71.3%\n",
      "Minibatch loss at step 650: 0.730480\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 75.7%\n",
      "Minibatch loss at step 700: 1.062416\n",
      "Minibatch accuracy: 60.9%\n",
      "Validation accuracy: 75.6%\n",
      "Minibatch loss at step 750: 0.889148\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 800: 0.687576\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 75.8%\n",
      "Minibatch loss at step 850: 0.792574\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 900: 0.774570\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 950: 0.980786\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 1000: 1.073451\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 80.8%\n",
      "Test accuracy: 80.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (svhn.train_labels.shape[0] - batch_size)\n",
    "    batch_data = svhn.train_data[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = svhn.train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), svhn.test_labels[:num_test_samples]))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), svhn.test_labels[:num_test_samples]))\n",
    "\n",
    "\n",
    "  W1 = layer1_weights.eval(session)\n",
    "  B1 = layer1_biases.eval(session)\n",
    "  W2 = layer2_weights.eval(session)\n",
    "  B2 = layer2_biases.eval(session)\n",
    "  W3 = layer3_weights.eval(session)\n",
    "  B3 = layer3_biases.eval(session)\n",
    "  W4 = layer4_weights.eval(session)\n",
    "  B4 = layer4_biases.eval(session)\n",
    "  W5 = layer5_weights.eval(session)\n",
    "  B5 = layer5_biases.eval(session)\n",
    "    \n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "\n",
    "  input = tf.placeholder(tf.float32, shape=(1, image_size, image_size, num_channels), name=\"input\")\n",
    "\n",
    "  W1 = tf.constant(W1, name=\"W1\")\n",
    "  B1 = tf.constant(B1, name=\"B1\")\n",
    "  W2 = tf.constant(W2, name=\"W2\")\n",
    "  B2 = tf.constant(B2, name=\"B2\")\n",
    "  W3 = tf.constant(W3, name=\"W3\")\n",
    "  B3 = tf.constant(B3, name=\"B3\")\n",
    "  W4 = tf.constant(W4, name=\"W4\")\n",
    "  B4 = tf.constant(B4, name=\"B4\")\n",
    "  W5 = tf.constant(W5, name=\"W5\")\n",
    "  B5 = tf.constant(B5, name=\"B5\")\n",
    "\n",
    "  # Model.\n",
    "  def model(data, keep_prob=1):\n",
    "    conv = tf.nn.conv2d(data, W1, [1, 1, 1, 1], padding=padding)\n",
    "    hidden = tf.nn.relu(conv + B1)\n",
    "    pool = tf.nn.max_pool(hidden, [1, pool_size, pool_size, 1],[1, pool_stride, pool_stride, 1], padding=padding)\n",
    "    \n",
    "    conv = tf.nn.conv2d(pool, W2, [1, 1, 1, 1], padding=padding)\n",
    "    hidden = tf.nn.relu(conv + B2)\n",
    "    pool = tf.nn.max_pool(hidden,[1, pool_size, pool_size, 1],[1, pool_stride, pool_stride, 1], padding=padding)\n",
    "\n",
    "    shape = pool.get_shape().as_list()\n",
    "    reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    \n",
    "    hidden = tf.nn.dropout(tf.nn.relu(tf.matmul(reshape, W3) + B3), keep_prob)\n",
    "\n",
    "    hidden = tf.nn.dropout(tf.nn.relu(tf.matmul(hidden, W4) + B4), keep_prob)\n",
    "    \n",
    "    return tf.matmul(hidden, W5) + B5\n",
    "\n",
    "  output = tf.nn.softmax(model(input), name=\"output\")\n",
    "    \n",
    "  sess = tf.Session()\n",
    "  init = tf.initialize_all_variables()\n",
    "  sess.run(init)\n",
    "\n",
    "  graph_def = g.as_graph_def()\n",
    "  tf.train.write_graph(graph_def, '/tmp', '4_convolutions_model_graph.pb', as_text=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
